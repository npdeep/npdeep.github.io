<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Cartpole Balancing Problem without Reinforcement Learning | Bits & Qubits</title><meta name=keywords content><meta name=description content="Who needs reinforcement learning when you can get away with basic physics?"><meta name=author content><link rel=canonical href=http://npdeep.github.io/expositions/cart-pole/><link crossorigin=anonymous href=/assets/css/stylesheet.min.8b2173082ac777566603fa3349c9e6fb3d9999ef4542c3c09caf828d9eb5a089.css integrity="sha256-iyFzCCrHd1ZmA/ozScnm+z2Zme9FQsPAnK+CjZ61oIk=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.e85ad0406048e8176e1c7661b25d5c69297ddfe41dc4124cf75ecb99a4f7b3d1.js integrity="sha256-6FrQQGBI6BduHHZhsl1caSl93+QdxBJM917LmaT3s9E=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=http://npdeep.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://npdeep.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://npdeep.github.io/favicon-32x32.png><link rel=apple-touch-icon href=http://npdeep.github.io/apple-touch-icon.png><link rel=mask-icon href=http://npdeep.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.101.0"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-877BXHD6V6"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-877BXHD6V6",{anonymize_ip:!1})}</script><meta property="og:title" content="Cartpole Balancing Problem without Reinforcement Learning"><meta property="og:description" content="Who needs reinforcement learning when you can get away with basic physics?"><meta property="og:type" content="article"><meta property="og:url" content="http://npdeep.github.io/expositions/cart-pole/"><meta property="article:section" content="expositions"><meta property="article:published_time" content="2020-10-15T00:00:00+00:00"><meta property="article:modified_time" content="2020-10-15T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Cartpole Balancing Problem without Reinforcement Learning"><meta name=twitter:description content="Who needs reinforcement learning when you can get away with basic physics?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Technical Notes","item":"http://npdeep.github.io/expositions/"},{"@type":"ListItem","position":3,"name":"Cartpole Balancing Problem without Reinforcement Learning","item":"http://npdeep.github.io/expositions/cart-pole/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Cartpole Balancing Problem without Reinforcement Learning","name":"Cartpole Balancing Problem without Reinforcement Learning","description":"Who needs reinforcement learning when you can get away with basic physics?","keywords":[],"articleBody":" The cart-pole balancing problem is a prototypical problem for reinforcement learning. A pole is attached to a cart with a rotating joint. An agent has to balance the pole by moving the cart left or right.\nMost solutions in the internet seem to use reinforcement learning with complicated Keras model to solve this problem.\nIn this post, I will show that if you understand your system well enough, you don’t need any deep-learning schemes. In the case of cartpole balancing, all you need is some rudimentary knowledge of mechanics.\nThe state of the system at any time $t$ is given by $(x,v, \\theta, \\omega)$ with\n$x$ is the position of the cart. $v = \\dot{x}$ is the velocity of the cart. $\\theta$ is the angle of the pole relative to normal. $\\omega = \\dot{\\theta}$ is the angular velocity of the pole. With this, we can compute the positions of the cart and the pole at $t’$. The agent checks if the horizontal position of the cart is to the left of the horizontal position of the pole. If yes move the cart to the right, otherwise move the cart to the left.\nThe decision function looks like this:\ndef make_decision(x, v, theta, omega, L=1, dt=0.027): predicted_theta = theta + omega*dt predicted_x_cart = x + v*dt predicted_x_tip_pole = predicted_x_cart + np.sin(predicted_theta)*L return 0 if predicted_x_tip_pole \u003c predicted_x_cart else 1 You need to know the time-step $\\delta t$ and the length of the pole $L$. Through trial and errors, I saw that $\\delta t \\approx 0.027, L=1$ works great on my computer.\nLet us use this decision function to balance the pole in OpenAI’s Gym environment.\nimport gym import numpy as np import matplotlib.pyplot as plt env = gym.make(\"CartPole-v1\") MAX_SCORE = 500 n_episodes = 40 scores = [] for k in range(n_episodes): env.reset() step = 0 while True: step += 1 env.render() state = env.state action = make_decision(*state) _, _, terminal, _ = env.step(action) if terminal or step \u003e= MAX_SCORE: break scores.append(step) There you go. You have balanced the cartpole without using any reinforcement learning.\n","wordCount":"343","inLanguage":"en","datePublished":"2020-10-15T00:00:00Z","dateModified":"2020-10-15T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http://npdeep.github.io/expositions/cart-pole/"},"publisher":{"@type":"Organization","name":"Bits \u0026 Qubits","logo":{"@type":"ImageObject","url":"http://npdeep.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://npdeep.github.io accesskey=h title="Bits & Qubits (Alt + H)">Bits & Qubits</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=http://npdeep.github.io/expositions/ title=Expositions><span>Expositions</span></a></li></ul></nav><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Cartpole Balancing Problem without Reinforcement Learning</h1><div class=post-meta><span title='2020-10-15 00:00:00 +0000 UTC'>October 15, 2020</span></div></header><div class=post-content><ul><li><p>The <a href=https://gym.openai.com/envs/CartPole-v0/>cart-pole balancing problem</a> is a prototypical problem for reinforcement learning. A pole is attached to a cart with a rotating joint. An agent has to balance the pole by moving the cart left or right.</p></li><li><p>Most solutions in the internet seem to use reinforcement learning with complicated Keras model to solve this problem.</p></li><li><p>In this post, I will show that if you understand your system well enough, you don&rsquo;t need any deep-learning schemes. In the case of cartpole balancing, all you need is some rudimentary knowledge of mechanics.</p></li></ul><p><img loading=lazy src=/expositions/cart-pole-0.png alt></p><p>The state of the system at any time $t$ is given by $(x,v, \theta, \omega)$ with</p><ul><li>$x$ is the position of the cart.</li><li>$v = \dot{x}$ is the velocity of the cart.</li><li>$\theta$ is the angle of the pole relative to normal.</li><li>$\omega = \dot{\theta}$ is the angular velocity of the pole.</li></ul><p>With this, we can compute the positions of the cart and the pole at $t&rsquo;$. The agent checks if the horizontal position of the cart is to the left of the horizontal position of the pole. If yes move the cart to the right, otherwise move the cart to the left.</p><p>The decision function looks like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>make_decision</span>(x, v, theta, omega, L<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, dt<span style=color:#f92672>=</span><span style=color:#ae81ff>0.027</span>):
</span></span><span style=display:flex><span>    predicted_theta <span style=color:#f92672>=</span> theta <span style=color:#f92672>+</span> omega<span style=color:#f92672>*</span>dt
</span></span><span style=display:flex><span>    predicted_x_cart <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> v<span style=color:#f92672>*</span>dt
</span></span><span style=display:flex><span>    predicted_x_tip_pole <span style=color:#f92672>=</span> predicted_x_cart <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>sin(predicted_theta)<span style=color:#f92672>*</span>L
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>if</span> predicted_x_tip_pole <span style=color:#f92672>&lt;</span> predicted_x_cart <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>1</span>
</span></span></code></pre></div><p>You need to know the time-step $\delta t$ and the length of the pole $L$. Through trial and errors, I saw that $\delta t \approx 0.027, L=1$ works great on my computer.</p><p>Let us use this decision function to balance the pole in OpenAI&rsquo;s Gym environment.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> gym
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>env <span style=color:#f92672>=</span> gym<span style=color:#f92672>.</span>make(<span style=color:#e6db74>&#34;CartPole-v1&#34;</span>)    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>MAX_SCORE <span style=color:#f92672>=</span> <span style=color:#ae81ff>500</span>
</span></span><span style=display:flex><span>n_episodes <span style=color:#f92672>=</span> <span style=color:#ae81ff>40</span>
</span></span><span style=display:flex><span>scores <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(n_episodes):
</span></span><span style=display:flex><span>    env<span style=color:#f92672>.</span>reset()
</span></span><span style=display:flex><span>    step <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> 
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        step <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        env<span style=color:#f92672>.</span>render()
</span></span><span style=display:flex><span>        state <span style=color:#f92672>=</span> env<span style=color:#f92672>.</span>state
</span></span><span style=display:flex><span>        action <span style=color:#f92672>=</span> make_decision(<span style=color:#f92672>*</span>state)
</span></span><span style=display:flex><span>        _, _, terminal, _ <span style=color:#f92672>=</span> env<span style=color:#f92672>.</span>step(action)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> terminal <span style=color:#f92672>or</span> step <span style=color:#f92672>&gt;=</span> MAX_SCORE:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>    scores<span style=color:#f92672>.</span>append(step)
</span></span></code></pre></div><p><img loading=lazy src=/expositions/cart-pole-1.gif alt></p><p>There you go. You have balanced the cartpole without using any reinforcement learning.</p></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2022 <a href=http://npdeep.github.io>Bits & Qubits</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>